{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Synthetic Data Fit\n\nThe IEC-recommended spectral tensor model is calibrated to fit the Kaimal spectra.\nThere are three free parameters: $L, T, C$, which have been precomputed in\n[Mann's original work](https://www.sciencedirect.com/science/article/pii/S0266892097000362)\nto be $L=0.59, T=3.9, C=3.2$, which will be used to compare against a DRD model fit.\nIn this example, the exponent $\\nu=-\\frac{1}{3}$ is fixed so that \n$\\tau(\\boldsymbol{k})$ matches the slope of $\\tau^{IEC}$ for \n$k \\rightarrow 0$. \n\nThe following example is also discussed in the [original DRD paper](https://arxiv.org/abs/2107.11046). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import packages\n\nFirst, we import the packages we need for this example. Additionally, we choose to use\nCUDA if it is available.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\n\nfrom drdmannturb import EddyLifetimeType\nfrom drdmannturb.parameters import (\n    LossParameters,\n    NNParameters,\n    PhysicalParameters,\n    ProblemParameters,\n)\nfrom drdmannturb.spectra_fitting import CalibrationProblem, OnePointSpectraDataGenerator\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# v2: torch.set_default_device('cuda:0')\nif torch.cuda.is_available():\n    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Physical Parameters\nThe following cell sets the necessary physical constants, including the characteristic\nscales for non-dimensionalization, the reference velocity, and the domain.\n\n$L$ is our characteristic length scale, $\\Gamma$ is our characteristic\ntime scale, and $\\sigma$ is the spectrum amplitude.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Characteristic scales associated with Kaimal spectrum\nL = 0.59  # length scale\nGamma = 3.9  # time scale\nsigma = 3.2  # energy spectrum scale\n\nUref = 21.0  # reference velocity\n\nzref = 1  # reference height\n\n# We consider the range :math:`\\mathcal{D} =[0.1, 100]` and sample the data points :math:`f_j \\in \\mathcal{D}` using a logarithmic grid of :math:`20` nodes.\ndomain = torch.logspace(-1, 2, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``CalibrationProblem`` construction\n\nWe'll use a simple neural network consisting of two layers with $10$ neurons each,\nconnected by a ReLU activation function. The parameters determining the network\narchitecture can conveniently be set through the ``NNParameters`` dataclass.\n\nUsing the ``ProblemParameters`` dataclass, we indicate the eddy lifetime function\n$\\tau$ substitution, that we do not intend to learn the exponent $\\nu$,\nand that we would like to train for 10 epochs, or until the tolerance ``tol`` loss (0.001 by default),\nwhichever is reached first.\n\nHaving set our physical parameters above, we need only pass these to the\n``PhysicalParameters`` dataclass just as is done below.\n\nLastly, using the ``LossParameters`` dataclass, we introduce a second-order\nderivative penalty term with weight $\\alpha_2 = 1$ and a\nnetwork parameter regularization term with weight\n$\\beta=10^{-5}$ to our MSE loss function.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pb = CalibrationProblem(\n    nn_params=NNParameters(\n        nlayers=2,\n        # Specifying the hidden layer sizes is done by passing a list of integers, as seen here.\n        hidden_layer_sizes=[10, 10],\n        # Specifying the activations is done similarly.\n        activations=[nn.ReLU(), nn.ReLU()],\n    ),\n    prob_params=ProblemParameters(\n        nepochs=10, learn_nu=False, eddy_lifetime=EddyLifetimeType.TAUNET\n    ),\n    # Note that we have not activated the first order term, but this can be done by passing a value for ``alpha_pen1``\n    loss_params=LossParameters(alpha_pen2=1.0, beta_reg=1.0e-5),\n    phys_params=PhysicalParameters(\n        L=L, Gamma=Gamma, sigma=sigma, Uref=Uref, domain=domain\n    ),\n    logging_directory=\"runs/synthetic_fit\",\n    device=device,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Generation\nIn the following cell, we construct our $k_1$ data points grid and\ngenerate the values. ``Data`` will be a tuple ``(<data points>, <data values>)``.\nIt is worth noting that the second element of each tuple in ``DataPoints`` is the corresponding\nreference height, which we have chosen to be uniformly `zref`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Data = OnePointSpectraDataGenerator(zref=zref, data_points=domain).Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calibration\nNow, we fit our model. ``CalibrationProblem.calibrate`` takes the tuple ``Data``\nwhich we just constructed and performs a typical training loop.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimal_parameters = pb.calibrate(data=Data)\n\npb.print_calibrated_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting\n``DRDMannTurb`` offers built-in plotting utilities and Tensorboard integration\nwhich make visualizing results and various aspects of training performance\nvery simple.\n\nThe following will plot our fit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pb.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This plots out the loss function terms as specified, each multiplied by the\nrespective coefficient hyperparameter. The training logs can be accessed from the logging directory\nwith Tensorboard utilities, but we also provide a simple internal utility for a single\ntraining log plot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pb.plot_losses(run_number=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Model with Problem Metadata\nHere, we'll make use of the model saving utilities,\nwhich make saving your ``DRDMannTurb`` fit very straightforward. The following line\nautomatically pickles and writes out a trained model along with the various\nparameter dataclasses in ``../results``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pb.save_model(\"../results/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Model and Problem Metadata\nLastly, we load our model back in.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pickle\n\npath_to_parameters = \"../results/EddyLifetimeType.TAUNET_DataType.KAIMAL.pkl\"\n\nwith open(path_to_parameters, \"rb\") as file:\n    (\n        nn_params,\n        prob_params,\n        loss_params,\n        phys_params,\n        model_params,\n    ) = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recovering Old Model Configuration and Old Parameters\nWe can also load the old model configuration from file and create a new ``CalibrationProblem`` object from the\nstored network parameters and metadata.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pb_new = CalibrationProblem(\n    nn_params=nn_params,\n    prob_params=prob_params,\n    loss_params=loss_params,\n    phys_params=phys_params,\n    device=device,\n)\n\npb_new.parameters = model_params\n\nimport numpy as np\n\nassert np.ma.allequal(pb.parameters, pb_new.parameters)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}